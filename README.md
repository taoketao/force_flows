# force_flows
To find functions that lie 'partway' between f(x)=x and g(x)=e^x, this project simulates flows according to approximations of force diagrams inspired by physics dynamical systems. The following README has the important points summarizing the work and context but is by no means thorough -- please ask me about specific things you're wondering about! Or wait for my more fleshed-out writeups.

**Context, motivation, and the Rho problem.** (todo: a proper writeup about the following concepts.) Neuromodulators such as dopamine and serotonin, which act as secondary messengers via G-proteins, change the binding affinities of primary neurotransmitter which act via ionotropic receptors. Established neural network models only attempt to model the action of primary neurotransmitters, so there's a gap in both modelling accuracy and potential computational complexity that are extant and obviously important to actual brain function and its ability to do amazing things such as generalize learned skills to new scenarios. Primary messenger action is well-modeled by the dot product or sum units. But to model neuromodulatory neurotransmitters in a neural network, simple sum units (*y_i* = *w_i^T* [dot] *x*) do not suffice, because they suffer from the 'echo' problem where any one neuron has negligible influence on outputs as the number of interconnected neurons grows. Something akin to multiplication (*y_i* = *w_i^T* \[dot\] (*x^(1)* [elementwise-mult] *x^(2)*) is more appropriate for modelling and utilizing secondary messengers. Modern machine learning models have begun implicitly implementing functional units with multiplications in them. They're aptly called 'gating units' -- of which LSTMs are a classic example -- and for a survey on so-called higher-order networks using multiplication units, see [https://taoketao.github.io/docs/Neuroscience_Final_Paper.pdf]). However, multiplication units also suffer from a complimentary 'veto' problem, where a single neuron outputting a value of zero annihilates the information conveyed by all the rest of the neurons. Neither of these models are biologically plausible, either -- any neuromodulator has significant but not total control over the receptors they modulate. To resolve these fundamental issues, I've been seeking a function 'between' addition and multiplication. This is in terms of both big-O complexity and also constituents making up intermediate complexities, such as the multiplication of N and log(N) in O(N\*logN). Such a solution I call 'Rho', which is the Greek letter between Sigma (addition) and Pi (multiplication). Once a suitable function is found, there are numerous ways to restructure a neural network. For several reasons including x\*y = exp{1}(log{1}(x)+log{1}(y) and x+y=exp{0}(log{0}(x)+log{0}(y), where f{N}(x) refers to N iterates of function f(x) such that f{3}(x)=f(f(f(x))), the problem of finding a function between addition and multiplication reduces to finding a half-exponential function, exp{1/2}. Such a problem has been approached by Godfrey and Gasler 2015 [https://arxiv.org/abs/1602.01321] and by Urban and Smagt 2016 [https://arxiv.org/abs/1503.05724], but both report methods that are shockingly unsatisfying, in that their interpolation functions (a) rely on parameters that the network itself learns thus admitting the 'veto' problem (todo: rigorous explanation) and (b) are not monotonic interpolations of the fractional iterates of the exponential function (see their plots of 3\~7 and 2\~7 respectively where \~ marks their interpolation). The general problem of finding a half-exponential function, or compositional square root of exp(x), is an interesting one, because it's not readily available: for example, finding a satisfying exp{\beta} for \beta>1 or \beta<0 is easier than finding an exp{\alpha}, 0<\alpha<1; it's been shown that such a half-exponential function would imply find a cardinality that is between the countable infinity |N| and the uncountable infinity |R|, which would resolve the continuum hypothesis, a problem that is certainly undecidable in ZFC theory; finding a functional root of Lambert's W function; and numerous other examples which are listed at [https://taoketao.github.io/rho-thoughts.html]). However, a child can see that you can draw a line between the Identity and Exponential functions. And from an information theoretic standpoint and the theory of homotopies, such functions ought to exist. (Ex: [https://web.archive.org/web/20210716130446/https://en.wikipedia.org/wiki/Half-exponential_function]). However, it's clear that neuromodulators systematically do calculations between addition and multiplication; while this may reduce to numerical changes in situ, I'm interested in finding a function that isn't limitied by numerical changes, instead qualitatively performing operations in a Rho regime as both a possible model of neural activity as well as a new kind of deep neural network designed for generalization.

In pursuit of finding half-exponential functions, I've found a new promising model inspired by physics and that applies to dynamical systems. Dynamical systems and neural networks have found mutual ground before, especially in regard to grid cells [McNaughton todo], recurrent networks & attractors [Hopfield 80s todo], and spin glasses [Amit 85 todo]; example work of mine in this aforementioned direction can be found at [https://github.com/taoketao/random_matrices_and_eigenvalues].) The model in this project uses dynamical systems tools in a different way. It forgoes algebraic manipulations (such as in [https://taoketao.github.io/docs/precis-connectionism-project.pdf]), any appeal to information theoretic constraints on neuron information, any spectral analysis (for now), or tricks involving dimension manipulation or reliance on data distributions, which are all plausible ways of approaching the question of finding a Rho function. Instead, it considers the following scenario:

**Model.** We look at 2D plots. Consider the lines f(x)=x and g(x)=exp(x) to have positive electromagnetic charges spread uniformly on the curve. (For now that means distributing equal charge per x-axis interval (x+\epsilon, x-\epsilon) for all x and \epsilon. todo: charge distributed evenly on the curve itself.) Look at all the points between these two curves as having a negative charge. Then consider physics actions on these points over time. Are there any fixed points that are motionless? Is there a curve for which any point on the curve stays on it over time, and if so, do the point charges tend to flow in one direction or another? Which force functions converge and which diverge? These questions are best solved by integrals, but since some of the integrals are nefarious (ie, integral of dx/(1+(e^x-x)^2) is not readily solvable to my knowledge), I'm running some simulations that approximate the integral with discrete sums to get an intuition to the possibilities. 

**Main problem.** Given f(x) and g(x), for the points in R^2 between *z*=\[u,v\] (ie, where for a given u, u\<v\<e^u), calculate the force on a given point according to the following physics-inspired force calculation.
- Pick a distance metric, such as the classic inverse-square law of gravity and electromagentism, d(*a*,*b*) = 1/(sqrt((a1-b1)^2 + (a2-b2)^2))
- For each point along *w*=\[x,f(x)\] and along \[x,g(x)\] (concatenated), calculate the force on *z* by *w* via formula d(*w*,*z*) * (*w*-*z*) / L2(*w*,*z), where L2 is the l-2 norm used to normalize direction vectors to unit length.
- Accumulate all the forces from across each *w*.
- *Either*: draw a force field plot where each point \[u,v\] is drawn an angle with a color corresponding to force strength,
- *Or*: simulate the trajectories of randomly selected points \[u,v\] and see which points do not converge to a point along \[x,f(x)\] or \[x,g(x)\].
- *Or*: run a small convolution resembling a Moore or von Neumann neighborhood (see: cellular automata) of the force field of \[u,v\] unit direction vectors over itself. Observe which points have the largest dot product to see which points form an unstable-equilibrium line on which points do not descend to \[x,f(x)\] or \[x,g(x)\]
- *Or*: analyze the spectra of operators which {are? aren't?} linear that send points to one of \[x,f(x)\] or \[x,g(x)\]. The 2D-eigenvectors are the points which don't land on these lines. (Note, points sitting on the Exp or Id function lines will probably not remain on that line (perhaps almost surely) 
- If possible, make the numerical calculations precise enough to inform an analytical solution for the whole R^2 plane or for smaller regions in it.
- Next, test different metrics. Perhaps, try varying the Lp exponent to see where the integrated forces start to converge, or try alternatively change distance measures to decrease not with squared distance but with, say, exponential or 'e^(-x^2)' or '(1-|x|)*indicator(|x|<1)' in distance. The question of different metrics is a secondary pursuit to understanding what happens with the straightforward L2 case.
